* finer feature construction

up til 04-02, root has form="</s>" upos="ROOT", missing nodes has form=""
upos="_", and unknown fields are treated as missing. the model cannot distinguish
missing nodes from nodes with missing info.

distinguish three special cases:

|         | dumb | root     | obsc |
|---------+------+----------+------|
| form    |      | </s>     | _    |
| upostag |      | ROOT     | _    |
| feats   |      | Root=Yes | _    |

in =d+r+o2=, ignore obsc-feat and set obsc-upos embedding to zero, since they can
never be trained.

fa

| 04-02       | d+r         | d+r+o       | d+r+o2      |
|-------------+-------------+-------------+-------------|
| 78.92 74.13 | 79.00 74.45 | 76.43 72.10 | 74.93 70.91 |
| 80.60 76.57 | 81.23 77.19 | 81.40 77.44 | 80.08 76.12 |
| 80.44 76.72 | 80.08 76.32 | 81.91 77.76 | 82.13 78.05 |
| 81.69 77.98 | 81.97 77.88 | 81.87 77.88 | 80.70 76.62 |
| 82.16 78.23 | 82.46 78.49 | 82.36 78.54 | 81.46 77.61 |
| 82.58 78.75 | 82.35 78.22 | 81.87 78.14 | 82.86 78.96 |
| 82.66 78.51 | 81.45 77.55 | 82.02 78.13 | 82.52 78.58 |
| 82.96 78.95 | 82.24 78.34 | 81.84 78.18 | 82.07 78.10 |
| 81.54 77.82 | 82.74 79.06 | 82.71 78.87 | 82.76 79.00 |
| 81.95 77.97 | 82.20 78.34 | 82.40 78.63 | 83.48 79.62 |

grc_proiel

| 04-02       | d+r         | d+r+o       | d+r+o2      |
|-------------+-------------+-------------+-------------|
| 74.37 70.46 | 75.51 71.26 | 73.25 69.22 | 74.99 70.99 |
| 77.64 73.48 | 76.66 72.69 | 77.12 72.95 | 77.78 73.59 |
| 78.35 74.32 | 77.83 73.91 | 77.40 73.63 | 77.62 73.70 |
| 77.82 73.81 | 78.04 74.14 | 78.38 74.55 | 78.97 75.26 |
| 79.41 75.58 | 78.89 74.95 | 78.74 74.90 | 78.44 74.75 |
| 79.34 75.58 | 78.76 74.88 | 78.20 74.55 | 78.13 74.30 |
| 78.93 75.19 | 78.89 75.18 | 79.01 75.11 | 78.90 75.07 |
| 78.42 74.36 | 79.15 75.10 | 79.25 75.55 | 79.20 75.45 |
| 78.66 74.84 | 78.41 74.66 | 78.74 74.89 | 78.71 74.91 |
| 79.40 75.58 | 79.45 75.72 | 78.38 74.55 | 78.12 74.37 |

** conclusion

good

* try label embedding

fa

| 10          | 15          | 20          | 25          |
|-------------+-------------+-------------+-------------|
| 78.19 73.68 | 79.43 74.92 | 78.57 74.25 | 79.16 74.82 |
| 81.83 77.73 | 80.03 76.01 | 81.64 77.67 | 79.71 75.73 |
| 81.67 77.40 | 82.47 78.49 | 80.38 76.54 | 82.14 77.95 |
| 82.08 78.14 | 82.54 78.57 | 82.37 78.40 | 82.05 78.34 |
| 82.40 78.41 | 82.66 78.87 | 82.33 78.27 | 82.06 78.26 |
| 81.17 77.27 | 82.84 78.82 | 82.50 78.66 | 82.04 77.98 |
| 80.89 77.15 | 83.30 79.62 | 83.43 79.46 | 83.07 79.00 |
| 82.61 78.99 | 82.91 79.26 | 83.03 79.30 | 82.06 78.11 |
| 82.86 78.92 | 83.03 78.92 | 82.78 78.87 | 82.63 78.76 |
| 83.03 79.38 | 82.86 79.01 | 82.64 78.71 | 82.72 78.82 |

** conclusion

good

* optimize activation

fa

| tanh        | relu        | leaky       | elu         |
|-------------+-------------+-------------+-------------|
| 79.43 74.92 | 76.71 72.18 | 78.39 73.89 | 78.92 74.66 |
| 80.03 76.01 | 81.57 77.20 | 81.53 77.60 | 80.91 76.73 |
| 82.47 78.49 | 82.43 78.48 | 82.22 78.15 | 80.45 76.47 |
| 82.54 78.57 | 83.58 79.60 | 82.42 78.65 | 81.75 77.83 |
| 82.66 78.87 | 82.79 78.87 | 82.53 78.67 | 82.00 78.09 |
| 82.84 78.82 | 82.66 78.80 | 82.59 78.83 | 82.34 78.51 |
| 83.30 79.62 | 81.11 77.25 | 82.88 79.16 | 82.51 78.58 |
| 82.91 79.26 | 83.21 79.30 | 82.21 78.45 | 82.04 77.73 |
| 83.03 78.92 | 83.35 79.48 | 82.56 78.71 | 81.76 77.87 |
| 82.86 79.01 | 83.47 79.64 | 83.26 79.62 | 82.43 78.74 |

with relu

| adagrad     | adadelta    | rmsprop     | adam        | adamax      | nadam       |
|-------------+-------------+-------------+-------------+-------------+-------------|
| 78.40 73.91 | 77.49 72.21 | 75.16 69.25 | 78.19 73.65 | 76.71 72.18 | 78.56 73.26 |
| 79.43 75.11 | 79.91 75.42 | 71.27 66.49 | 80.75 76.36 | 81.57 77.20 | 80.27 75.39 |
| 80.01 75.82 | 80.12 75.59 | 73.74 68.34 | 81.89 77.60 | 82.43 78.48 | 79.90 74.96 |
| 80.05 75.95 | 82.34 78.00 | 77.26 71.86 | 81.02 76.67 | 83.58 79.60 | 78.72 73.86 |
| 81.42 77.34 | 81.15 77.10 | 74.37 68.90 | 82.63 78.48 | 82.79 78.87 | 78.97 74.16 |
| 81.27 77.24 | 82.16 77.65 | 76.60 70.81 | 81.06 76.64 | 82.66 78.80 | 79.62 74.73 |
| 81.02 77.11 | 81.91 77.80 | 73.04 67.37 | 81.06 76.76 | 81.11 77.25 | 80.08 75.18 |
| 81.23 77.35 | 83.05 78.91 | 75.45 69.47 | 81.56 77.22 | 83.21 79.30 | 78.83 74.17 |
| 81.40 77.47 | 82.59 78.49 | 78.23 72.29 | 81.83 77.33 | 83.35 79.48 | 79.75 74.93 |
| 81.23 77.41 | 82.43 78.35 | 75.49 70.45 | 81.31 76.92 | 83.47 79.64 | 79.83 74.89 |

** conclusion

relu seems to be less stable but more promising. try relu from now on.

* try valency feature again

fa

| binary      | freq        |
|-------------+-------------|
| 77.77 72.68 | 78.33 73.98 |
| 79.88 75.40 | 80.63 76.50 |
| 82.18 78.26 | 81.97 77.86 |
| 81.28 77.34 | 82.25 78.24 |
| 82.78 78.83 | 82.48 78.56 |
| 82.49 78.82 | 82.86 79.07 |
| 83.04 79.13 | 82.47 78.42 |
| 82.72 78.97 | 82.55 78.63 |
| 82.86 78.87 | 82.86 78.86 |
| 83.31 79.45 | 83.29 79.51 |

** conclusion

not trying again

* optimize embedding initializer

fa

| uniform     | normal      | truncated-normal |
|-------------+-------------+------------------|
| 78.12 73.65 | 79.15 74.34 | 79.75 75.22      |
| 81.87 77.43 | 81.30 77.09 | 79.91 75.81      |
| 81.39 77.27 | 82.17 78.08 | 81.25 77.42      |
| 82.23 78.31 | 82.26 78.46 | 82.81 78.74      |
| 83.02 79.02 | 83.26 79.31 | 82.88 79.18      |
| 83.18 79.42 | 81.43 77.59 | 82.67 78.57      |
| 82.62 78.95 | 82.82 79.01 | 83.10 79.26      |
| 82.48 78.72 | 83.12 79.25 | 83.62 79.84      |
| 83.11 79.18 | 83.38 79.52 | 83.91 80.12      |
| 82.86 79.02 | 83.36 79.50 | 82.89 79.07      |

** conclusion

truncated-normal

* try two hidden layers of 200 each

fa

| 79.98 74.87 |
| 81.52 77.37 |
| 80.73 76.67 |
| 82.99 79.28 |
| 83.92 79.98 |
| 81.17 77.34 |
| 82.67 78.59 |
| 83.01 79.15 |
| 82.64 78.71 |
| 82.04 77.84 | 

** conclusion

looks promising. seems to be over-fitting too soon. come back to this after
having dropout figured out.

* optimize hidden initializer

fa

| glorot_uniform | glorot_normal | he_uniform  | he_normal   | lecun_uniform |
|----------------+---------------+-------------+-------------+---------------|
| 78.68 74.17    | 78.09 73.38   | 78.82 74.41 | 78.16 73.45 | 78.08 73.26   |
| 81.54 77.55    | 79.71 75.75   | 80.26 76.11 | 81.37 77.28 | 81.58 77.28   |
| 82.44 78.27    | 81.75 77.63   | 81.76 77.81 | 82.76 78.72 | 81.48 77.30   |
| 83.18 79.13    | 82.67 78.70   | 82.28 78.18 | 82.57 78.62 | 82.19 78.25   |
| 83.20 79.40    | 82.44 78.65   | 82.74 78.82 | 82.83 79.01 | 82.21 78.31   |
| 83.27 79.32    | 81.69 77.58   | 82.33 78.44 | 82.90 79.00 | 82.86 78.94   |
| 82.71 78.84    | 82.90 78.77   | 82.87 79.01 | 82.42 78.38 | 83.94 80.14   |
| 83.38 79.51    | 82.67 78.69   | 82.65 78.99 | 82.44 78.55 | 82.63 78.94   |
| 82.69 78.90    | 82.05 78.31   | 82.80 79.00 | 81.88 77.95 | 84.03 80.17   |
| 81.36 77.29    | 82.87 78.87   | 81.92 78.15 | 81.16 76.95 | 82.16 78.30   |

** conclusion

glorot-uniform

* optimize output initializer

fa

????

** conclusion

????

* optimize embedding dims

????

** conclusion

????










